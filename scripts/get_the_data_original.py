import os
from sre_parse import CATEGORIES
import torch
import pandas as pd
from skimage import io #transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.data.sampler import SubsetRandomSampler
from torchvision import transforms, utils
import itertools
import math as m
from PIL import Image

from config_gen import get_config
import csv
import os

object_labels = [
    "car",
    'motorcycle',
    'truck',
    'tractor',
    "bus",
    "bicycle",
]
n_objects = len(object_labels)

location_labels = [
    "grass",
    "city",
    "desert",
    "snow",
]
n_locations = len(location_labels)

time_labels = [
    "day",
    "night",
]
n_times = len(time_labels)

# Set parameters
csv_file = "gen_images/csv_files/24-Sep__21-03.csv"
root_dir = "gen_images/24-Sep__21-03__array-1/"
default_specs = {'train_causal_factors': {'object': object_labels,
                                        'location': location_labels,
                                        'time_of_day': time_labels},
                'train_len': 500,
                'valid_causal_factors': {'object': object_labels,
                                        'location': location_labels,
                                        'time_of_day': time_labels},
                'valid_len': 300,
                'test_causal_factors': {'object': object_labels,
                                        'location': location_labels,
                                        'time_of_day': time_labels},
                'test_len': 200
                }


# Adapted from pytorch tutorial
class CausalDiffDataset(Dataset):
    """Causal dataset."""

    def __init__(self, csv_file, root_dir, transform=None, 
                ):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.causal_diff_frame = pd.read_csv(csv_file)
        self.root_dir = root_dir
        self.transform = transform

    def __len__(self):
        return len(self.causal_diff_frame)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        img_name = os.path.join(self.root_dir,
                                self.causal_diff_frame.iloc[idx, 0])
        image = Image.open(img_name)
        labels = self.causal_diff_frame.iloc[idx, 1:]
        object = labels[0]
        location = labels[1]
        time_of_day = labels[2]

        sample = {'image': image, 
                'labels': [object, location, time_of_day]
                }

        if self.transform:
            sample['image'] = self.transform(image)

        return sample

def intersection(lst1, lst2):
    lst3 = [value for value in lst1 if value in lst2]
    return lst3

def get_train_valid_test_loaders(csv_file=csv_file, root_dir=root_dir, 
                                batch_size=256, num_workers=2, 
                                specs=default_specs,
                                augment=True):


    """
    This function returns train, valid and test loaders where the train, valid and test 
    datasets are generated by separate causal factors. The factors that generate the 
    splits, and how large each dataset is, are specified in the specs dictionary.

    Further, to mitigate any confounding between the generating factors, this function asserts
    that the distribution of instances for each causal category is the same between splits,
    aside from the chosen environmental discrepancies

    If there is no causal discrepancy between the train, valid and test, we will still obtain mutually
    separate datasets, with the proportions of causal factors identical between the datasets

    args:
        - csv file (string)
        - root_dir (string)
        - batch_size (int)
        - num_workers (int)
        - specs (dict) - default specs is given above

    output:
        train_loader, valid_loader, test_loader
    """

    # Define normalize transform
    normalize = transforms.Normalize(
        mean=[0.4914, 0.4822, 0.4465],
        std=[0.2023, 0.1994, 0.2010],
    )

    # define transforms
    valid_transform = transforms.Compose([
            transforms.ToTensor(),
            normalize,
    ])
    if augment:
        train_transform = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            normalize,
            ])
    else:
        train_transform = transforms.Compose([
            transforms.ToTensor(),
            normalize,
        ])

    test_transform = transforms.Compose([
            transforms.ToTensor(),
            normalize,
    ])
   
    # Load datasets
    train_dataset = CausalDiffDataset(csv_file, root_dir, 
                                    transform=train_transform)
    valid_dataset = CausalDiffDataset(csv_file, root_dir, 
                                    transform=valid_transform)
    test_dataset = CausalDiffDataset(csv_file, root_dir, 
                                    transform=test_transform)

    # Specify causal factors and nunber of elements for each dataset
    train_causal_factors = specs['train_causal_factors']
    train_len = specs['train_len']
    valid_causal_factors = specs['valid_causal_factors']
    valid_len = specs['valid_len']
    test_causal_factors = specs['test_causal_factors']
    test_len = specs['test_len']
    
    # work out combinations permitted in each dataset, and introduce a dictionary with 
    # counters for each combination
    train_factor_combinations = list(itertools.product(
        train_causal_factors['object'],
        train_causal_factors['location'],
        train_causal_factors['time_of_day']
    ))
    n_train_combinations = len(train_factor_combinations)
    train_combinations_counter_dict = {}
    for item in train_factor_combinations:
        train_combinations_counter_dict[item] = 0

    valid_factor_combinations = list(itertools.product(
        valid_causal_factors['object'],
        valid_causal_factors['location'],
        valid_causal_factors['time_of_day']
    ))
    n_valid_combinations = len(valid_factor_combinations)
    valid_combinations_counter_dict = {}
    for item in valid_factor_combinations:
        valid_combinations_counter_dict[item] = 0

    test_factor_combinations = list(itertools.product(
        test_causal_factors['object'],
        test_causal_factors['location'],
        test_causal_factors['time_of_day']
    ))
    n_test_combinations = len(test_factor_combinations)
    test_combinations_counter_dict = {}
    for item in test_factor_combinations:
        test_combinations_counter_dict[item] = 0

    # work out the number of permitted combinations in each dataset
    n_each_train_comb = m.floor(train_len / n_train_combinations)
    n_each_valid_comb = m.floor(valid_len / n_valid_combinations)
    n_each_test_comb = m.floor(test_len / n_test_combinations)

    # build a list of permitted indices for each dataset
    train_indices = []
    valid_indices = []
    test_indices = []
    for idx in range(len(train_dataset)):

        object_label = train_dataset[idx]['labels'][0]
        location_label = train_dataset[idx]['labels'][1]
        time_label = train_dataset[idx]['labels'][2]

        # check for permissible causal factors combination
        if object_label in train_causal_factors['object']:
            if location_label in train_causal_factors['location']:
                if time_label in train_causal_factors['time_of_day']:
                    # check for counter to keep data balanced across causal factor combinations
                    if train_combinations_counter_dict[(object_label, location_label, time_label)] < n_each_train_comb:
                        
                        # add index to list
                        train_indices.append(idx)
                        # add to counter for given combination
                        train_combinations_counter_dict[(object_label, location_label, time_label)] += 1

        # make sure we don't have overlapping index lists
        if idx not in train_indices:
            if object_label in valid_causal_factors['object']:
                if location_label in valid_causal_factors['location']:
                    if time_label in valid_causal_factors['time_of_day']:
                        if valid_combinations_counter_dict[(object_label, location_label, time_label)] < n_each_valid_comb:
                            
                            valid_indices.append(idx)
                            valid_combinations_counter_dict[(object_label, location_label, time_label)] += 1

        if idx not in train_indices and idx not in valid_indices:
            if object_label in test_causal_factors['object']:
                if location_label in test_causal_factors['location']:
                    if time_label in test_causal_factors['time_of_day']:
                        if test_combinations_counter_dict[(object_label, location_label, time_label)] < n_each_test_comb:
                            
                            test_indices.append(idx)
                            test_combinations_counter_dict[(object_label, location_label, time_label)] += 1

    # make sure the training, test and valid sets do not overlap
    assert len(intersection(train_indices, valid_indices)) == 0 
    assert len(intersection(train_indices, test_indices)) == 0
    assert len(intersection(test_indices, valid_indices)) == 0

    # make sure the proportions of combinations are equal
    for key in train_combinations_counter_dict:
        assert train_combinations_counter_dict[key] == n_each_train_comb
    
    for key in valid_combinations_counter_dict:
        assert valid_combinations_counter_dict[key] == n_each_valid_comb

    for key in test_combinations_counter_dict:
        assert test_combinations_counter_dict[key] == n_each_test_comb

    # Get train, valid and test datasets
    train_dataset = Subset(train_dataset, train_indices)
    valid_dataset = Subset(valid_dataset, valid_indices)
    test_dataset = Subset(test_dataset, test_indices)

    # Get train, valid and test loaders    
    train_loader = DataLoader(train_dataset, shuffle=True,
                            batch_size=batch_size,
                            num_workers=num_workers)
    valid_loader = DataLoader(valid_dataset, shuffle=True,
                            batch_size=batch_size,
                            num_workers=num_workers)
    test_loader = DataLoader(test_dataset, shuffle=False,
                            batch_size=batch_size,
                            num_workers=num_workers)
    
    return train_loader, valid_loader, test_loader


train_loader, valid_loader, test_loader = get_train_valid_test_loaders()



